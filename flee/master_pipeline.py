"""
í†µí•© ë§ˆìŠ¤í„° íŒŒì´í”„ë¼ì¸
í¬ë¡¤ë§ â†’ LLM ì •ì œ â†’ ë¡œì»¬ DB â†’ Supabase ì €ì¥ê¹Œì§€ ì „ì²´ ìë™í™”
"""
import sys
import os
from datetime import datetime
import json
import logging
from pathlib import Path
from flea_list_fast import main as crawl_list


# ì¸ì½”ë”© ì„¤ì • (ì•ˆì „í•˜ê²Œ ì²˜ë¦¬)
if sys.platform == "win32":
    import io
    # stdoutì´ ì´ë¯¸ TextIOWrapperê°€ ì•„ë‹ˆê³  bufferê°€ ìˆëŠ” ê²½ìš°ë§Œ ì¬ì„¤ì •
    if hasattr(sys.stdout, 'buffer') and not isinstance(sys.stdout, io.TextIOWrapper):
        try:
            # bufferê°€ ìœ íš¨í•œì§€ í™•ì¸
            if sys.stdout.buffer and not sys.stdout.buffer.closed:
                sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')
        except (AttributeError, ValueError, OSError):
            # ì‹¤íŒ¨ ì‹œ ê¸°ì¡´ stdout ìœ ì§€
            pass

# ë¡œê·¸ í´ë” ìƒì„±
LOG_DIR = Path("logs")
LOG_DIR.mkdir(exist_ok=True)

# ë¡œê¹… ì„¤ì •
log_filename = LOG_DIR / f"pipeline_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log"
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s [%(levelname)s] %(message)s',
    handlers=[
        logging.FileHandler(log_filename, encoding='utf-8'),
        logging.StreamHandler(sys.stdout)
    ]
)
logger = logging.getLogger(__name__)

# ì‹¤í–‰ í†µê³„ ì €ì¥ìš©
class PipelineStats:
    def __init__(self):
        self.start_time = None
        self.end_time = None
        self.steps = {
            'crawling': {'success': None, 'error': None, 'duration': None},
            'llm_processing': {'success': None, 'error': None, 'duration': None},
            'local_db': {'success': None, 'error': None, 'duration': None},
            'supabase': {'success': None, 'error': None, 'duration': None, 'success_count': 0, 'fail_count': 0}
        }

    def save_report(self, output_file="summary.txt"):
        """ì‹¤í–‰ ë¦¬í¬íŠ¸ ì €ì¥"""
        with open(output_file, "w", encoding="utf-8") as f:
            f.write("=" * 80 + "\n")
            f.write("í”Œë¦¬ë§ˆì¼“ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ë¦¬í¬íŠ¸\n")
            f.write("=" * 80 + "\n\n")

            f.write(f"ì‹œì‘ ì‹œê°„: {self.start_time.strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"ì¢…ë£Œ ì‹œê°„: {self.end_time.strftime('%Y-%m-%d %H:%M:%S')}\n")

            duration = (self.end_time - self.start_time).total_seconds()
            f.write(f"ì´ ì†Œìš” ì‹œê°„: {duration:.1f}ì´ˆ ({duration/60:.1f}ë¶„)\n\n")

            f.write("-" * 80 + "\n")
            f.write("ë‹¨ê³„ë³„ ê²°ê³¼\n")
            f.write("-" * 80 + "\n\n")

            for step_name, step_data in self.steps.items():
                if step_data['success'] is not None:
                    status = "âœ… ì„±ê³µ" if step_data['success'] else "âŒ ì‹¤íŒ¨"
                    f.write(f"[{step_name.upper()}] {status}\n")

                    if step_data['duration']:
                        f.write(f"  ì†Œìš” ì‹œê°„: {step_data['duration']:.1f}ì´ˆ\n")

                    if step_data['error']:
                        f.write(f"  ì˜¤ë¥˜: {step_data['error']}\n")

                    if step_name == 'supabase' and step_data['success']:
                        f.write(f"  ì„±ê³µ: {step_data['success_count']}ê°œ\n")
                        f.write(f"  ì‹¤íŒ¨: {step_data['fail_count']}ê°œ\n")

                    f.write("\n")

            f.write("=" * 80 + "\n")
            f.write(f"ë¡œê·¸ íŒŒì¼: {log_filename}\n")
            f.write("=" * 80 + "\n")

stats = PipelineStats()


def print_header(title, char="="):
    """í—¤ë” ì¶œë ¥"""
    print()
    print(char * 80)
    print(f"  {title}")
    print(char * 80)
    print()


def print_step(step_num, title):
    """ë‹¨ê³„ ì¶œë ¥"""
    print()
    print(f"{'='*80}")
    print(f"  STEP {step_num}: {title}")
    print(f"{'='*80}")
    print()


def run_crawling():
    """í¬ë¡¤ë§ ì‹¤í–‰"""
    print_step(1, "í”Œë¦¬ë§ˆì¼“ í¬ë¡¤ë§")
    step_start = datetime.now()

    try:
        from flea_list_fast import main as crawl_list
        from flea_text_fast import main as crawl_detail

        logger.info("ê²Œì‹œë¬¼ ëª©ë¡ í¬ë¡¤ë§ ì‹œì‘")
        crawl_list()
        logger.info("ëª©ë¡ í¬ë¡¤ë§ ì™„ë£Œ")

        logger.info("ìƒì„¸ ê²Œì‹œê¸€ í¬ë¡¤ë§ ì‹œì‘")
        crawl_detail()
        logger.info("ìƒì„¸ í¬ë¡¤ë§ ì™„ë£Œ")

        duration = (datetime.now() - step_start).total_seconds()
        stats.steps['crawling']['success'] = True
        stats.steps['crawling']['duration'] = duration

        return True
    except Exception as e:
        logger.error(f"í¬ë¡¤ë§ ì‹¤íŒ¨: {e}", exc_info=True)

        duration = (datetime.now() - step_start).total_seconds()
        stats.steps['crawling']['success'] = False
        stats.steps['crawling']['error'] = str(e)
        stats.steps['crawling']['duration'] = duration

        return False


def run_llm_processing(force_update=False):
    """LLM ë°ì´í„° ì •ì œ"""
    print_step(2, "LLM ë°ì´í„° ì •ì œ")
    step_start = datetime.now()

    try:
        from extract_to_json import extract_to_json

        logger.info("LLM ì •ì œ ì‹œì‘ (gpt-4o-mini)")
        result = extract_to_json("fleamarket_detail.json", force_update=force_update)

        duration = (datetime.now() - step_start).total_seconds()

        if result:
            logger.info("LLM ì •ì œ ì™„ë£Œ")
            stats.steps['llm_processing']['success'] = True
            stats.steps['llm_processing']['duration'] = duration
            return True
        else:
            logger.error("LLM ì •ì œ ì‹¤íŒ¨")
            stats.steps['llm_processing']['success'] = False
            stats.steps['llm_processing']['error'] = "extract_to_json returned None"
            stats.steps['llm_processing']['duration'] = duration
            return False

    except Exception as e:
        logger.error(f"LLM ì •ì œ ì˜¤ë¥˜: {e}", exc_info=True)

        duration = (datetime.now() - step_start).total_seconds()
        stats.steps['llm_processing']['success'] = False
        stats.steps['llm_processing']['error'] = str(e)
        stats.steps['llm_processing']['duration'] = duration

        return False


def save_to_local_db():
    """ë¡œì»¬ SQLite DB ì €ì¥"""
    print_step(3, "ë¡œì»¬ DB ì €ì¥")
    step_start = datetime.now()

    try:
        from structured_to_db import structured_to_db

        logger.info("ë¡œì»¬ SQLite DB ì €ì¥ ì‹œì‘")
        structured_to_db("fleamarket_structured.json", skip_existing=False)
        logger.info("ë¡œì»¬ DB ì €ì¥ ì™„ë£Œ")

        duration = (datetime.now() - step_start).total_seconds()
        stats.steps['local_db']['success'] = True
        stats.steps['local_db']['duration'] = duration

        return True
    except Exception as e:
        logger.error(f"ë¡œì»¬ DB ì €ì¥ ì‹¤íŒ¨: {e}", exc_info=True)

        duration = (datetime.now() - step_start).total_seconds()
        stats.steps['local_db']['success'] = False
        stats.steps['local_db']['error'] = str(e)
        stats.steps['local_db']['duration'] = duration

        return False


def save_to_supabase():
    """Supabase ì €ì¥"""
    print_step(4, "Supabase ì €ì¥")
    step_start = datetime.now()

    try:
        from supabase_manager import save_to_db

        # structured.json ì½ê¸°
        with open("fleamarket_structured.json", "r", encoding="utf-8") as f:
            structured_data = json.load(f)

        logger.info(f"{len(structured_data)}ê°œ ë°ì´í„° Supabase ì €ì¥ ì‹œì‘")

        success_count = 0
        fail_count = 0

        for i, data in enumerate(structured_data, 1):
            market_name = data.get("market_name", "ì œëª© ë¯¸ì •")
            image_url = data.get("_source", {}).get("image_url", "")

            print(f"[{i}/{len(structured_data)}] {market_name[:50]}")

            if save_to_db(data, image_url):
                success_count += 1
            else:
                fail_count += 1

        print()
        print("-" * 80)
        logger.info(f"Supabase ì €ì¥ ì™„ë£Œ - ì„±ê³µ: {success_count}ê°œ, ì‹¤íŒ¨: {fail_count}ê°œ")
        print("-" * 80)

        duration = (datetime.now() - step_start).total_seconds()
        stats.steps['supabase']['success'] = success_count > 0
        stats.steps['supabase']['duration'] = duration
        stats.steps['supabase']['success_count'] = success_count
        stats.steps['supabase']['fail_count'] = fail_count

        return success_count > 0

    except FileNotFoundError:
        logger.error("fleamarket_structured.json íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤")

        duration = (datetime.now() - step_start).total_seconds()
        stats.steps['supabase']['success'] = False
        stats.steps['supabase']['error'] = "fleamarket_structured.json íŒŒì¼ì´ ì—†ìŒ"
        stats.steps['supabase']['duration'] = duration

        return False
    except Exception as e:
        logger.error(f"Supabase ì €ì¥ ì˜¤ë¥˜: {e}", exc_info=True)

        duration = (datetime.now() - step_start).total_seconds()
        stats.steps['supabase']['success'] = False
        stats.steps['supabase']['error'] = str(e)
        stats.steps['supabase']['duration'] = duration

        return False


def main(skip_crawling=False, skip_llm=False, force_update=False):
    """
    ë©”ì¸ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰

    Args:
        skip_crawling: í¬ë¡¤ë§ ë‹¨ê³„ ê±´ë„ˆë›°ê¸°
        skip_llm: LLM ì •ì œ ë‹¨ê³„ ê±´ë„ˆë›°ê¸° (structured.json ì¬ì‚¬ìš©)
        force_update: ê¸°ì¡´ ë°ì´í„° ë®ì–´ì“°ê¸°
    """
    stats.start_time = datetime.now()

    print_header("ğŸš€ í”Œë¦¬ë§ˆì¼“ í†µí•© íŒŒì´í”„ë¼ì¸", "=")
    logger.info(f"íŒŒì´í”„ë¼ì¸ ì‹œì‘ - ëª¨ë“œ: {'ì „ì²´ ì¬ì²˜ë¦¬' if force_update else 'ì¦ë¶„ ì—…ë°ì´íŠ¸'}")
    print(f"â° ì‹œì‘ ì‹œê°„: {stats.start_time.strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"ğŸ“Œ ëª¨ë“œ: {'ì „ì²´ ì¬ì²˜ë¦¬' if force_update else 'ì¦ë¶„ ì—…ë°ì´íŠ¸'}")

    # Step 1: í¬ë¡¤ë§
    if not skip_crawling:
        if not run_crawling():
            logger.error("í¬ë¡¤ë§ ì‹¤íŒ¨ë¡œ íŒŒì´í”„ë¼ì¸ ì¤‘ë‹¨")
            print("\nâŒ í¬ë¡¤ë§ ì‹¤íŒ¨ë¡œ íŒŒì´í”„ë¼ì¸ ì¤‘ë‹¨")
            stats.end_time = datetime.now()
            stats.save_report()
            return False
    else:
        print_step(1, "í¬ë¡¤ë§ ê±´ë„ˆë›°ê¸°")
        logger.info("í¬ë¡¤ë§ ê±´ë„ˆë›°ê¸° - ê¸°ì¡´ ë°ì´í„° ì‚¬ìš©")
        print("â­ï¸  ê¸°ì¡´ í¬ë¡¤ë§ ë°ì´í„° ì‚¬ìš©")

    # Step 2: LLM ì •ì œ
    if not skip_llm:
        if not run_llm_processing(force_update):
            logger.error("LLM ì •ì œ ì‹¤íŒ¨ë¡œ íŒŒì´í”„ë¼ì¸ ì¤‘ë‹¨")
            print("\nâŒ LLM ì •ì œ ì‹¤íŒ¨ë¡œ íŒŒì´í”„ë¼ì¸ ì¤‘ë‹¨")
            stats.end_time = datetime.now()
            stats.save_report()
            return False
    else:
        print_step(2, "LLM ì •ì œ ê±´ë„ˆë›°ê¸°")
        logger.info("LLM ì •ì œ ê±´ë„ˆë›°ê¸° - ê¸°ì¡´ structured.json ì‚¬ìš©")
        print("â­ï¸  ê¸°ì¡´ structured.json ì‚¬ìš©")

    # Step 3: ë¡œì»¬ DB ì €ì¥
    if not save_to_local_db():
        logger.warning("ë¡œì»¬ DB ì €ì¥ ì‹¤íŒ¨ (ê³„ì† ì§„í–‰)")
        print("\nâš ï¸  ë¡œì»¬ DB ì €ì¥ ì‹¤íŒ¨ (ê³„ì† ì§„í–‰)")

    # Step 4: Supabase ì €ì¥
    if not save_to_supabase():
        logger.error("Supabase ì €ì¥ ì‹¤íŒ¨")
        print("\nâŒ Supabase ì €ì¥ ì‹¤íŒ¨")
        stats.end_time = datetime.now()
        stats.save_report()
        return False

    # ì™„ë£Œ
    stats.end_time = datetime.now()
    duration = (stats.end_time - stats.start_time).total_seconds()

    # ë¦¬í¬íŠ¸ ì €ì¥
    stats.save_report()

    print_header("ğŸ‰ íŒŒì´í”„ë¼ì¸ ì™„ë£Œ!", "=")
    logger.info(f"íŒŒì´í”„ë¼ì¸ ì™„ë£Œ - ì†Œìš” ì‹œê°„: {duration:.1f}ì´ˆ")
    print(f"â° ì¢…ë£Œ ì‹œê°„: {stats.end_time.strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"â±ï¸  ì†Œìš” ì‹œê°„: {duration:.1f}ì´ˆ")
    print()
    print(f"ğŸ“Š ì‹¤í–‰ ë¦¬í¬íŠ¸: summary.txt")
    print(f"ğŸ“‹ ë¡œê·¸ íŒŒì¼: {log_filename}")
    print()

    return True


if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser(description="í”Œë¦¬ë§ˆì¼“ í†µí•© íŒŒì´í”„ë¼ì¸")
    parser.add_argument("--skip-crawling", action="store_true", help="í¬ë¡¤ë§ ë‹¨ê³„ ê±´ë„ˆë›°ê¸°")
    parser.add_argument("--skip-llm", action="store_true", help="LLM ì •ì œ ë‹¨ê³„ ê±´ë„ˆë›°ê¸°")
    parser.add_argument("--force", "-f", action="store_true", help="ì „ì²´ ì¬ì²˜ë¦¬ ëª¨ë“œ")

    args = parser.parse_args()

    success = main(
        skip_crawling=args.skip_crawling,
        skip_llm=args.skip_llm,
        force_update=args.force
    )

    sys.exit(0 if success else 1)
