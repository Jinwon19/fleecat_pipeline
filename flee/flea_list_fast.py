"""
í”Œë¦¬ë§ˆì¼“ ê²Œì‹œë¬¼ ëª©ë¡ í¬ë¡¤ë§ (ë³‘ë ¬ ì²˜ë¦¬ ê³ ì† ë²„ì „)
- requests ìš°ì„  ì‹œë„ (ë¹ ë¦„)
- ì‹¤íŒ¨ì‹œ Selenium Headlessë¡œ ìë™ ì „í™˜
"""
import requests
from bs4 import BeautifulSoup
import json
import os
from concurrent.futures import ThreadPoolExecutor, as_completed
from tqdm import tqdm
import time
import sys

# Windows ì¸ì½”ë”© ë¬¸ì œ í•´ê²°
if sys.platform == "win32":
    import io
    try:
        if hasattr(sys.stdout, 'buffer') and sys.stdout.buffer:
            sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')
    except (AttributeError, ValueError):
        pass  # subprocessë¡œ ì‹¤í–‰ë  ë•ŒëŠ” ìŠ¤í‚µ

# ==================== ì„¤ì • ====================
BASE_URL = "https://xn--oy2b2b112gxof.com/"
OUTPUT_FILE = "fleamarket_posts.json"
MAX_WORKERS = 5  # ë™ì‹œ ì²˜ë¦¬ í˜ì´ì§€ ìˆ˜
MAX_PAGES = 10   # í¬ë¡¤ë§í•  ìµœëŒ€ í˜ì´ì§€ ìˆ˜ (í•„ìš”ì‹œ ìˆ˜ì •)
RETRY_ATTEMPTS = 3

HEADERS = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
    'Accept-Language': 'ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7',
}


# ==================== requests ë°©ì‹ ====================
def fetch_page_requests(page_num):
    """requestsë¡œ íŠ¹ì • í˜ì´ì§€ í¬ë¡¤ë§"""
    url = f"{BASE_URL}?page={page_num}" if page_num > 1 else BASE_URL
    
    for attempt in range(RETRY_ATTEMPTS):
        try:
            response = requests.get(url, headers=HEADERS, timeout=10)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.text, 'html.parser')
            cards = soup.select('.col-xs-6.col-sm-3.col-md-3.item')
            
            if not cards:
                return None  # ì¹´ë“œ ì—†ìœ¼ë©´ ì‹¤íŒ¨
            
            posts = []
            for card in cards:
                try:
                    title_elem = card.select_one('.tpl-forum-list-title')
                    img_elem = card.select_one('img')
                    link_elem = card.select_one('a')
                    
                    if title_elem and link_elem:
                        posts.append({
                            "title": title_elem.text.strip(),
                            "image_url": img_elem['src'] if img_elem and img_elem.get('src') else "",
                            "link": link_elem['href']
                        })
                except Exception as e:
                    continue
            
            return posts
            
        except Exception as e:
            if attempt == RETRY_ATTEMPTS - 1:
                print(f"âŒ í˜ì´ì§€ {page_num} ìš”ì²­ ì‹¤íŒ¨ (ì¬ì‹œë„ {attempt+1}/{RETRY_ATTEMPTS}): {e}")
                return None
            time.sleep(1)
    
    return None


# ==================== Selenium ë°©ì‹ (ë°±ì—…) ====================
def fetch_page_selenium(page_num):
    """Selenium Headlessë¡œ íŠ¹ì • í˜ì´ì§€ í¬ë¡¤ë§"""
    from selenium import webdriver
    from selenium.webdriver.chrome.options import Options
    from selenium.webdriver.chrome.service import Service
    from selenium.webdriver.common.by import By
    
    chrome_options = Options()
    chrome_options.add_argument('--headless')  # ì°½ ì•ˆ ë„ì›€
    chrome_options.add_argument('--disable-gpu')
    chrome_options.add_argument('--no-sandbox')
    chrome_options.add_argument('--disable-dev-shm-usage')
    chrome_options.add_argument(f'user-agent={HEADERS["User-Agent"]}')
    
    try:
        # chromedriver ê²½ë¡œ (í˜„ì¬ ë””ë ‰í† ë¦¬)
        service = Service(r"C:\Users\yg-603-20\Desktop\ì—°ìŠµ í”„ë¡œì íŠ¸\í”Œë¦¬ë§ˆì¼“\flee\chromedriver.exe")
        driver = webdriver.Chrome(service=service, options=chrome_options)
        
        url = BASE_URL
        driver.get(url)
        time.sleep(2)
        
        # í˜ì´ì§€ ì´ë™ (2í˜ì´ì§€ ì´ìƒ)
        if page_num > 1:
            for _ in range(page_num - 1):
                try:
                    next_btn = driver.find_element(By.LINK_TEXT, str(_ + 2))
                    next_btn.click()
                    time.sleep(2)
                except:
                    driver.quit()
                    return None
        
        # ì¹´ë“œ ìˆ˜ì§‘
        cards = driver.find_elements(By.CSS_SELECTOR, '.col-xs-6.col-sm-3.col-md-3.item')
        
        posts = []
        for card in cards:
            try:
                title = card.find_element(By.CSS_SELECTOR, '.tpl-forum-list-title').text.strip()
                img = card.find_element(By.CSS_SELECTOR, 'img').get_attribute('src')
                link = card.find_element(By.CSS_SELECTOR, 'a').get_attribute('href')
                
                posts.append({
                    "title": title,
                    "image_url": img or "",
                    "link": link
                })
            except:
                continue
        
        driver.quit()
        return posts
        
    except Exception as e:
        print(f"âŒ Selenium í˜ì´ì§€ {page_num} ì‹¤íŒ¨: {e}")
        try:
            driver.quit()
        except:
            pass
        return None


# ==================== ë©”ì¸ ë¡œì§ ====================
def crawl_all_pages():
    """ë³‘ë ¬ë¡œ ëª¨ë“  í˜ì´ì§€ í¬ë¡¤ë§"""

    # ê¸°ì¡´ ë°ì´í„° ë¡œë“œ
    if os.path.exists(OUTPUT_FILE):
        with open(OUTPUT_FILE, "r", encoding="utf-8") as f:
            existing_posts = json.load(f)
        existing_links = {p["link"] for p in existing_posts}
        print(f"ğŸ“‚ ê¸°ì¡´ ê²Œì‹œë¬¼ {len(existing_posts)}ê°œ ë¡œë“œ")
    else:
        existing_posts = []
        existing_links = set()

    # ì²« í˜ì´ì§€ë¡œ requests í…ŒìŠ¤íŠ¸
    print("ğŸ” requests ë°©ì‹ í…ŒìŠ¤íŠ¸ ì¤‘...")
    test_result = fetch_page_requests(1)

    if test_result is None:
        print("âš ï¸ requests ì‹¤íŒ¨ â†’ Selenium Headlessë¡œ ì „í™˜")
        use_selenium = True
    else:
        print(f"âœ… requests ì„±ê³µ! ({len(test_result)}ê°œ ì¹´ë“œ ë°œê²¬)")
        use_selenium = False

    # ë³‘ë ¬ í¬ë¡¤ë§
    all_new_posts = []

    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
        if use_selenium:
            futures = {executor.submit(fetch_page_selenium, page): page for page in range(1, MAX_PAGES + 1)}
        else:
            futures = {executor.submit(fetch_page_requests, page): page for page in range(1, MAX_PAGES + 1)}

        print(f"\nğŸš€ {MAX_PAGES}ê°œ í˜ì´ì§€ ë³‘ë ¬ í¬ë¡¤ë§ ì‹œì‘ (ë™ì‹œ {MAX_WORKERS}ê°œ)")

        for future in tqdm(as_completed(futures), total=len(futures), desc="í¬ë¡¤ë§ ì§„í–‰"):
            page_num = futures[future]
            try:
                posts = future.result()
                if posts:
                    # ì¤‘ë³µ ì œê±°
                    new_posts = [p for p in posts if p["link"] not in existing_links]
                    all_new_posts.extend(new_posts)
                    existing_links.update(p["link"] for p in new_posts)
                    # print(f"âœ… í˜ì´ì§€ {page_num}: {len(new_posts)}ê°œ ì‹ ê·œ")
            except Exception as e:
                print(f"âŒ í˜ì´ì§€ {page_num} ì²˜ë¦¬ ì˜¤ë¥˜: {e}")

    # ê²°ê³¼ ì €ì¥
    all_posts = existing_posts + all_new_posts

    with open(OUTPUT_FILE, "w", encoding="utf-8") as f:
        json.dump(all_posts, f, ensure_ascii=False, indent=2)

    print(f"\nâœ… í¬ë¡¤ë§ ì™„ë£Œ!")
    print(f"   ì´ ê²Œì‹œë¬¼: {len(all_posts)}ê°œ")
    print(f"   ì‹ ê·œ ì¶”ê°€: {len(all_new_posts)}ê°œ")
    print(f"   ì €ì¥ ìœ„ì¹˜: {OUTPUT_FILE}")


def main():
    """ë©”ì¸ í•¨ìˆ˜ - master_pipelineì—ì„œ í˜¸ì¶œ"""
    return crawl_all_pages()


if __name__ == "__main__":
    main()

