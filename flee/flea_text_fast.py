"""
í”Œë¦¬ë§ˆì¼“ ìƒì„¸ ê²Œì‹œê¸€ í¬ë¡¤ë§ (ë³‘ë ¬ ì²˜ë¦¬ ê³ ì† ë²„ì „)
- requests ìš°ì„  ì‹œë„ (ë¹ ë¦„)
- ì‹¤íŒ¨ì‹œ Selenium Headlessë¡œ ìë™ ì „í™˜
"""
import requests
from bs4 import BeautifulSoup
import json
import os
import re
from concurrent.futures import ThreadPoolExecutor, as_completed
from tqdm import tqdm
import time
import sys

# Windows ì¸ì½”ë”© ë¬¸ì œ í•´ê²°
if sys.platform == "win32":
    import io
    try:
        if hasattr(sys.stdout, 'buffer') and sys.stdout.buffer:
            sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')
    except (AttributeError, ValueError):
        pass  # subprocessë¡œ ì‹¤í–‰ë  ë•ŒëŠ” ìŠ¤í‚µ

# ==================== ì„¤ì • ====================
POSTS_FILE = "fleamarket_posts.json"
OUTPUT_FILE = "fleamarket_detail.json"
MAX_WORKERS = 10  # ë™ì‹œ ì²˜ë¦¬ í˜ì´ì§€ ìˆ˜ (ìƒì„¸í˜ì´ì§€ëŠ” ë” ë§ì´ ê°€ëŠ¥)
RETRY_ATTEMPTS = 3

HEADERS = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
    'Accept-Language': 'ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7',
}


# ==================== requests ë°©ì‹ ====================
def fetch_detail_requests(link):
    """requestsë¡œ ìƒì„¸ í˜ì´ì§€ í¬ë¡¤ë§"""
    
    for attempt in range(RETRY_ATTEMPTS):
        try:
            response = requests.get(link, headers=HEADERS, timeout=10)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # ê²Œì‹œê¸€ ë³¸ë¬¸ ì°¾ê¸° (ì „ì²´ HTML í™•ë³´)
            content_elem = soup.select_one('.fr-element.fr-view')
            if not content_elem:
                # ë‹¤ë¥¸ ê°€ëŠ¥í•œ ì„ íƒìë“¤ ì‹œë„
                content_elem = soup.select_one('.content') or soup.select_one('.post-content') or soup.select_one('article')

            if not content_elem:
                return None  # ì½˜í…ì¸  ì—†ìœ¼ë©´ ì‹¤íŒ¨

            # get_text()ë¥¼ ì‚¬ìš©í•˜ì—¬ ì „ì²´ í…ìŠ¤íŠ¸ ì¶”ì¶œ (200ì ì œí•œ ë°©ì§€)
            content_text = content_elem.get_text(strip=False, separator='\n')

            # í…ìŠ¤íŠ¸ ê¸¸ì´ í™•ì¸ (ë””ë²„ê¹…ìš©)
            if len(content_text) < 250:
                # 200ì ë¯¸ë§Œì´ë©´ Seleniumìœ¼ë¡œ ì¬ì‹œë„í•˜ëŠ” ê²ƒì´ ë‚˜ì„ ìˆ˜ ìˆìŒ
                # í•˜ì§€ë§Œ ì—¬ê¸°ì„œëŠ” ì¼ë‹¨ ì§„í–‰
                pass
            
            # ì œëª© ì¶”ì¶œ
            title_elem = soup.select_one('title') or soup.select_one('h1')
            title = title_elem.text.strip() if title_elem else ""
            
            # ì •ê·œì‹ìœ¼ë¡œ ì •ë³´ ì¶”ì¶œ
            market_name = re.search(r"í”„ë¦¬ë§ˆì¼“ëª…\s*[:ï¼š]\s*(.*)", content_text)
            date_time = re.search(r"ë‚ ì§œ.*[:ï¼š]\s*(.*)", content_text)
            place = re.search(r"ì¥ì†Œ\s*[:ï¼š]\s*(.*)", content_text)
            
            # ì´ë¯¸ì§€ URL ì¶”ì¶œ (ë³¸ë¬¸ ì²« ì´ë¯¸ì§€)
            img_elem = content_elem.select_one('img')
            image_url = img_elem['src'] if img_elem and img_elem.get('src') else ""

            # ê²Œì‹œê¸€ ì‘ì„±ì¼ ì¶”ì¶œ (ì—°ë„ ì¶”ë¡ ì— ì‚¬ìš©)
            post_date_elem = soup.select_one('.tpl-forum-date') or soup.select_one('.date')
            post_date = ""
            if post_date_elem:
                post_date_text = post_date_elem.get_text(strip=True)
                # "2025. 10. 2" í˜•ì‹ì—ì„œ ë‚ ì§œ ì¶”ì¶œ
                date_match = re.search(r'(\d{4})\.\s*(\d{1,2})\.\s*(\d{1,2})', post_date_text)
                if date_match:
                    year, month, day = date_match.groups()
                    post_date = f"{year}-{month.zfill(2)}-{day.zfill(2)}"

            result = {
                "url": link,
                "title": title,
                "market_name": market_name.group(1).strip() if market_name else "",
                "date_time": date_time.group(1).strip() if date_time else "",
                "place": place.group(1).strip() if place else "",
                "raw_text": content_text.strip(),
                "image_url": image_url,
                "post_date": post_date
            }
            
            return result
            
        except Exception as e:
            if attempt == RETRY_ATTEMPTS - 1:
                # print(f"âŒ {link} ìš”ì²­ ì‹¤íŒ¨: {e}")
                return None
            time.sleep(1)
    
    return None


# ==================== Selenium ë°©ì‹ (ë°±ì—…) ====================
def fetch_detail_selenium(link):
    """Selenium Headlessë¡œ ìƒì„¸ í˜ì´ì§€ í¬ë¡¤ë§"""
    from selenium import webdriver
    from selenium.webdriver.chrome.options import Options
    from selenium.webdriver.chrome.service import Service
    from selenium.webdriver.common.by import By
    
    chrome_options = Options()
    chrome_options.add_argument('--headless')
    chrome_options.add_argument('--disable-gpu')
    chrome_options.add_argument('--no-sandbox')
    chrome_options.add_argument('--disable-dev-shm-usage')
    chrome_options.add_argument(f'user-agent={HEADERS["User-Agent"]}')
    
    try:
        service = Service(r"C:\Users\yg-603-20\Desktop\ì—°ìŠµ í”„ë¡œì íŠ¸\í”Œë¦¬ë§ˆì¼“\flee\chromedriver.exe")
        driver = webdriver.Chrome(service=service, options=chrome_options)
        
        driver.get(link)
        time.sleep(5)  # ë™ì  ì½˜í…ì¸  ë¡œë”© ëŒ€ê¸° (2ì´ˆ â†’ 5ì´ˆë¡œ ì¦ê°€)

        # í˜ì´ì§€ í•˜ë‹¨ê¹Œì§€ ìŠ¤í¬ë¡¤í•˜ì—¬ ì „ì²´ ì»¨í…ì¸  ë¡œë“œ
        driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
        time.sleep(3)  # ì¶”ê°€ ì»¨í…ì¸  ë¡œë“œ ëŒ€ê¸° (1ì´ˆ â†’ 3ì´ˆë¡œ ì¦ê°€)

        # ê²Œì‹œê¸€ ë³¸ë¬¸ (ì „ì²´ HTML ê°€ì ¸ì˜¤ê¸°)
        try:
            content_elem = driver.find_element(By.CSS_SELECTOR, '.fr-element.fr-view')
            # .text ëŒ€ì‹  innerHTML ê°€ì ¸ì™€ì„œ ì „ì²´ ë‚´ìš© í™•ë³´
            content_html = content_elem.get_attribute('innerHTML')
            soup = BeautifulSoup(content_html, 'html.parser')
            content_text = soup.get_text(strip=False, separator='\n')
        except:
            try:
                content_elem = driver.find_element(By.CSS_SELECTOR, '.content')
                content_html = content_elem.get_attribute('innerHTML')
                soup = BeautifulSoup(content_html, 'html.parser')
                content_text = soup.get_text(strip=False, separator='\n')
            except:
                driver.quit()
                return None
        
        # ì •ê·œì‹ìœ¼ë¡œ ì •ë³´ ì¶”ì¶œ
        market_name = re.search(r"í”„ë¦¬ë§ˆì¼“ëª…\s*[:ï¼š]\s*(.*)", content_text)
        date_time = re.search(r"ë‚ ì§œ.*[:ï¼š]\s*(.*)", content_text)
        place = re.search(r"ì¥ì†Œ\s*[:ï¼š]\s*(.*)", content_text)

        # ê²Œì‹œê¸€ ì‘ì„±ì¼ ì¶”ì¶œ (ì—°ë„ ì¶”ë¡ ì— ì‚¬ìš©)
        post_date = ""
        try:
            # .tpl-forum-date ë˜ëŠ” .date ì„ íƒìë¡œ ì‘ì„±ì¼ ì°¾ê¸°
            post_date_elem = driver.find_element(By.CSS_SELECTOR, '.tpl-forum-date') if driver.find_elements(By.CSS_SELECTOR, '.tpl-forum-date') else None
            if not post_date_elem:
                post_date_elem = driver.find_element(By.CSS_SELECTOR, '.date') if driver.find_elements(By.CSS_SELECTOR, '.date') else None

            if post_date_elem:
                post_date_text = post_date_elem.text.strip()
                # "2025. 10. 2" í˜•ì‹ì—ì„œ ë‚ ì§œ ì¶”ì¶œ
                date_match = re.search(r'(\d{4})\.\s*(\d{1,2})\.\s*(\d{1,2})', post_date_text)
                if date_match:
                    year, month, day = date_match.groups()
                    post_date = f"{year}-{month.zfill(2)}-{day.zfill(2)}"
        except:
            pass  # ì‘ì„±ì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìœ¼ë©´ ë¹ˆ ë¬¸ìì—´

        result = {
            "url": link,
            "title": driver.title,
            "market_name": market_name.group(1).strip() if market_name else "",
            "date_time": date_time.group(1).strip() if date_time else "",
            "place": place.group(1).strip() if place else "",
            "raw_text": content_text.strip(),
            "image_url": "",
            "post_date": post_date
        }
        
        driver.quit()
        return result
        
    except Exception as e:
        # print(f"âŒ Selenium {link} ì‹¤íŒ¨: {e}")
        try:
            driver.quit()
        except:
            pass
        return None


# ==================== ë©”ì¸ ë¡œì§ ====================
def crawl_all_details():
    """ë³‘ë ¬ë¡œ ëª¨ë“  ìƒì„¸ í˜ì´ì§€ í¬ë¡¤ë§"""

    # posts íŒŒì¼ í™•ì¸
    if not os.path.exists(POSTS_FILE):
        print(f"âŒ {POSTS_FILE} íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € flea_list_fast.pyë¥¼ ì‹¤í–‰í•˜ì„¸ìš”.")
        return

    with open(POSTS_FILE, "r", encoding="utf-8") as f:
        posts = json.load(f)

    print(f"ğŸ“‚ {len(posts)}ê°œ ê²Œì‹œë¬¼ ë¡œë“œ")

    # ê¸°ì¡´ ìƒì„¸ ë°ì´í„° ë¡œë“œ
    if os.path.exists(OUTPUT_FILE):
        with open(OUTPUT_FILE, "r", encoding="utf-8") as f:
            existing_details = json.load(f)
        existing_urls = {d["url"] for d in existing_details}
        print(f"ğŸ“‚ ê¸°ì¡´ ìƒì„¸ ë°ì´í„° {len(existing_details)}ê°œ ë¡œë“œ")
    else:
        existing_details = []
        existing_urls = set()

    # í¬ë¡¤ë§í•  ë§í¬ í•„í„°ë§ (ì´ë¯¸ ìˆëŠ” ê²ƒ ì œì™¸)
    links_to_crawl = [p["link"] for p in posts if p["link"] not in existing_urls]

    if not links_to_crawl:
        print("âœ… ëª¨ë“  ê²Œì‹œë¬¼ì´ ì´ë¯¸ í¬ë¡¤ë§ë˜ì—ˆìŠµë‹ˆë‹¤.")
        return

    print(f"ğŸ¯ í¬ë¡¤ë§ ëŒ€ìƒ: {len(links_to_crawl)}ê°œ")

    # ì²« ë§í¬ë¡œ requests í…ŒìŠ¤íŠ¸
    print("ğŸ” requests ë°©ì‹ í…ŒìŠ¤íŠ¸ ì¤‘...")
    test_result = fetch_detail_requests(links_to_crawl[0])

    # ê²°ê³¼ê°€ Noneì´ê±°ë‚˜ í…ìŠ¤íŠ¸ ê¸¸ì´ê°€ 500ì ë¯¸ë§Œì´ë©´ Selenium ì‚¬ìš©
    if test_result is None:
        print("âš ï¸ requests ì‹¤íŒ¨ (ê²°ê³¼ ì—†ìŒ) â†’ Selenium Headlessë¡œ ì „í™˜")
        use_selenium = True
    elif len(test_result.get("raw_text", "")) < 500:
        print(f"âš ï¸ requestsë¡œ ë¶ˆì™„ì „í•œ ë°ì´í„° ìˆ˜ì§‘ (í…ìŠ¤íŠ¸ {len(test_result.get('raw_text', ''))}ì) â†’ Selenium Headlessë¡œ ì „í™˜")
        use_selenium = True
    else:
        print(f"âœ… requests ì„±ê³µ! (í…ìŠ¤íŠ¸ {len(test_result.get('raw_text', ''))}ì)")
        use_selenium = False

    # ë³‘ë ¬ í¬ë¡¤ë§
    all_new_details = []

    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
        if use_selenium:
            futures = {executor.submit(fetch_detail_selenium, link): link for link in links_to_crawl}
        else:
            futures = {executor.submit(fetch_detail_requests, link): link for link in links_to_crawl}

        print(f"\nğŸš€ {len(links_to_crawl)}ê°œ ìƒì„¸ í˜ì´ì§€ ë³‘ë ¬ í¬ë¡¤ë§ ì‹œì‘ (ë™ì‹œ {MAX_WORKERS}ê°œ)")

        for future in tqdm(as_completed(futures), total=len(futures), desc="í¬ë¡¤ë§ ì§„í–‰"):
            link = futures[future]
            try:
                detail = future.result()
                if detail:
                    all_new_details.append(detail)
            except Exception as e:
                print(f"âŒ {link} ì²˜ë¦¬ ì˜¤ë¥˜: {e}")

    # ê²°ê³¼ ì €ì¥
    all_details = existing_details + all_new_details

    with open(OUTPUT_FILE, "w", encoding="utf-8") as f:
        json.dump(all_details, f, ensure_ascii=False, indent=2)

    print(f"\nâœ… í¬ë¡¤ë§ ì™„ë£Œ!")
    print(f"   ì´ ìƒì„¸ ë°ì´í„°: {len(all_details)}ê°œ")
    print(f"   ì‹ ê·œ ì¶”ê°€: {len(all_new_details)}ê°œ")
    print(f"   ì €ì¥ ìœ„ì¹˜: {OUTPUT_FILE}")


def main():
    """ë©”ì¸ í•¨ìˆ˜ - master_pipelineì—ì„œ í˜¸ì¶œ"""
    return crawl_all_details()


if __name__ == "__main__":
    main()

